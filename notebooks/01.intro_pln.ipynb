{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aula 01 – Introdução ao PLN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "O **Processamento de Linguagem Natural (PLN)** é um campo da Inteligência Artificial que permite que máquinas entendam e interajam com linguagem humana.\n",
        "\n",
        "### Exemplos de uso:\n",
        "- Chatbots e assistentes virtuais (Siri, Alexa, ChatGPT)\n",
        "- Análise de sentimento em redes sociais\n",
        "- Corretores ortográficos e de gramática\n",
        "- Tradução automática"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instalação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AwEU7X_z_7Qb",
        "outputId": "83e63321-b3be-4ce8-c224-54959c0df996"
      },
      "outputs": [],
      "source": [
        "%pip install nltk spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importação de bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XEaDcP38Bk-j",
        "outputId": "8b09bd8e-ad0e-4014-88da-9a89205f8606"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Texto exemplo para análise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "252lcYsqCB9G",
        "outputId": "9d1e402f-37e8-4602-ad61-bd08f79cb07d"
      },
      "outputs": [],
      "source": [
        "texto = \"Mais vale um asno que me carregue que um cavalo que me derrube.\"\n",
        "print(texto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKsVSq7aGFUF"
      },
      "source": [
        "## Tokenização com NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwlmwXeRCHlq",
        "outputId": "4f7a9044-0b37-4c5b-f83e-b3d37a4ffbed"
      },
      "outputs": [],
      "source": [
        "tokens_nltk = word_tokenize(texto, language='portuguese')\n",
        "print(tokens_nltk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEF_4p26GL8u"
      },
      "source": [
        "## Remoção de Stopwords\n",
        "\n",
        "Stopwords são palavras comuns em textos, como \"o\", \"é\", \"sou\", que têm pouco valor semântico. Elas são filtradas no pré-processamento para melhorar a eficiência\n",
        "e a precisão nas tarefas de PLN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhQW5tcsDy8j",
        "outputId": "6c064587-5a2e-4295-da80-e888fcf2cfef"
      },
      "outputs": [],
      "source": [
        "stopwords_pt = stopwords.words('portuguese')\n",
        "filtered = [w for w in tokens_nltk if w not in stopwords_pt]\n",
        "print(filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLqR2YmoGdIq"
      },
      "source": [
        "## Stemming com NLTK\n",
        "\n",
        "Stemming são técnicas que reduz às suas raízes (radicais), cortando sufixos. Por exemplo, \"amado\", \"amarei\", viram \"am\". Obs.: o radical pode não ser uma palabra correta e não é a mesma coisa que o lema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMKC2cT-Fmip",
        "outputId": "ab8782db-dc1b-424f-efe6-ba6bcc869f58"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "stems = [stemmer.stem(word) for word in filtered]\n",
        "print(stems)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwJd77AuIbwM"
      },
      "source": [
        "## Instalação da lib\n",
        "\n",
        "> pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c9lsTWbcIKDn",
        "outputId": "c9a8a767-7d1d-4889-d551-aba5bc76c634"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU_oZh0xG4Po"
      },
      "source": [
        "## SpaCy – Tokenização e Lematização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8d1LbEKHHCh",
        "outputId": "2db0d205-9407-4b24-ea8f-c358f3dc5d69"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "doc = nlp(texto)\n",
        "\n",
        "print(f\"{'Texto':<10} | {'Lema':<10} | {'Gramática':<10}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"{token.text:<10} | {token.lemma_:<10} | {token.pos_:<10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Desafio prático"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "texto = \"O preço do café avança rapidamente, isso é preocupante.\"\n",
        "tokens = word_tokenize(texto.lower())\n",
        "tokens_filtrados = [w for w in tokens if w not in stopwords_pt]\n",
        "stems = [stemmer.stem(word) for word in tokens_filtrados]\n",
        "print(\"Original:\", texto, \"\\n\\nTokens:\", tokens, \"\\n\\nSem stopwords:\", tokens_filtrados, \"\\n\\nStems:\", stems)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
